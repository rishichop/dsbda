{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJZyxO2TmhVO9qDsBBpAdE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxSUgcSMa89Z","executionInfo":{"status":"ok","timestamp":1708414754909,"user_tz":-330,"elapsed":728,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"24006d40-5e12-4f84-edfc-722633917834"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import nltk\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"averaged_perceptron_tagger\")"]},{"cell_type":"code","source":["text = \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""],"metadata":{"id":"C3HIkUsLdC2b","executionInfo":{"status":"ok","timestamp":1708414788464,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"1y9OX9pmdX8E"}},{"cell_type":"code","source":["#Sentence Tokenization\n","from nltk.tokenize import sent_tokenize\n","\n","tokenized_text = sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAcdKbQPdNuO","executionInfo":{"status":"ok","timestamp":1708414892008,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"e477115e-a93c-4491-912b-b924c9849944"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"]}]},{"cell_type":"code","source":["#Word Tokenization\n","from nltk.tokenize import word_tokenize\n","\n","tokenized_word = word_tokenize(text)\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYzp2GwndS3s","executionInfo":{"status":"ok","timestamp":1708414965752,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"39e400da-b5a0-455c-c6ae-79189cb798ca"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"]}]},{"cell_type":"code","source":["#print stop words of English\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YviK3AYxd5Ew","executionInfo":{"status":"ok","timestamp":1708415226090,"user_tz":-330,"elapsed":496,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"8254aff9-1dbb-41c4-e527-5fbe902715bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'your', 'doesn', 'whom', 'should', 'against', 'other', 'those', 'down', 'mustn', 'yourself', \"couldn't\", \"should've\", 'weren', \"shan't\", 'is', 're', \"mightn't\", 'have', 'you', 'who', 'then', 'aren', 'shan', 'itself', 'll', 'wasn', 'not', 'couldn', 'before', \"weren't\", \"you'd\", 'ourselves', 'no', 'just', \"don't\", \"isn't\", 'been', 'more', 'but', 'it', 'myself', \"you're\", 'them', \"wasn't\", 'has', 'with', 'our', 'hadn', 'below', 'while', \"aren't\", 'of', 'ma', 'd', 'which', 'this', 'are', 'once', \"needn't\", 'own', 'there', 'won', 'both', \"wouldn't\", 'was', \"hadn't\", 'y', 'my', 'nor', 'or', \"mustn't\", 'such', 'they', \"that'll\", 'the', 'at', 'yours', 'under', 'he', 'so', 'if', \"you've\", 'i', 'what', 'didn', 'we', 'a', 'o', 'these', 'be', 'out', \"you'll\", 'his', 'hers', 'theirs', 'him', 'that', 'now', 'by', 'mightn', \"it's\", 'their', \"hasn't\", 'did', 've', 'few', 'in', 'himself', 'do', 'only', 'needn', 'ain', 'very', 'being', 'again', 'how', 'am', 'and', 'an', 'same', 'doing', 'her', 'as', 'after', 'wouldn', 'isn', 'each', 'ours', 'all', 's', 'hasn', 'to', 'haven', 'than', 'were', \"haven't\", 'why', 'me', 'between', 'she', 'above', 'will', 'its', 'over', 'any', 'some', 'shouldn', \"she's\", 'from', \"doesn't\", 'themselves', 'too', 'does', 't', 'when', 'until', \"won't\", 'most', 'yourselves', 'm', 'here', 'further', \"shouldn't\", 'herself', 'on', 'having', 'into', 'because', 'up', 'for', 'during', \"didn't\", 'where', 'about', 'through', 'had', 'can', 'off', 'don'}\n"]}]},{"cell_type":"code","source":["import re\n","text = \"How to remove stop words with NLTK library in Python?\""],"metadata":{"id":"xaFBbGHJeicJ","executionInfo":{"status":"ok","timestamp":1708415414618,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["text = re.sub('[^a-zA-Z]', ' ', text)\n","print(text)\n","tokens = word_tokenize(text.lower())\n","filtered_text = []\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"tokenized Sentence:\", tokens)\n","print(\"Filtered Sentence:\", filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghvsvp8Wfm2f","executionInfo":{"status":"ok","timestamp":1708419205260,"user_tz":-330,"elapsed":520,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"dddc37ee-3e4f-4021-d030-d83db9cccf5b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["How to remove stop words with NLTK library in Python \n","tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n","Filtered Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"]}]},{"cell_type":"code","source":["#Stemming\n","from nltk.stem import PorterStemmer\n","e_words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps = PorterStemmer()\n","for w in e_words:\n","  rootWord = ps.stem(w)\n","  print(rootWord)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKH9IURpt1ct","executionInfo":{"status":"ok","timestamp":1708419621545,"user_tz":-330,"elapsed":417,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"56959bb3-f2f9-4358-ac7e-024a0c2d59f6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n","wait\n","wait\n","wait\n"]}]},{"cell_type":"code","source":["#Lemmatization\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daA-5luQumAV","executionInfo":{"status":"ok","timestamp":1708419588264,"user_tz":-330,"elapsed":1702,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"9e8ddbfd-4e77-4519-92b8-3f8744d16c29"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jq7UTavMu_rd"},"execution_count":null,"outputs":[]}]}