{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcaP/FbzjoOtBmLn3pED/q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JcO8Pp4QsPna","executionInfo":{"status":"ok","timestamp":1714794093027,"user_tz":-330,"elapsed":2064,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.probability import FreqDist\n","from nltk.tag import pos_tag\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOPrxQzSsUyr","executionInfo":{"status":"ok","timestamp":1714794103752,"user_tz":-330,"elapsed":1845,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"2106d30f-9c2d-419c-824d-d84c13043bc6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Sample document\n","sample_document = \"\"\"Natural language processing (NLP) is a subfield of artificial intelligence (AI) that\n","focuses on the interaction between computers and humans using natural language. It enables computers\n","to understand, interpret, and generate human-like text. NLP involves various tasks, such as tokenization,\n","part-of-speech tagging, stop words removal, stemming, and lemmatization.\"\"\""],"metadata":{"id":"eR4O3voEsWYZ","executionInfo":{"status":"ok","timestamp":1714794110413,"user_tz":-330,"elapsed":428,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","tokens = word_tokenize(sample_document)"],"metadata":{"id":"gweltN78sZhh","executionInfo":{"status":"ok","timestamp":1714794116890,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# POS Tagging\n","pos_tags = pos_tag(tokens)"],"metadata":{"id":"tPk5blyqsbLA","executionInfo":{"status":"ok","timestamp":1714794122259,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Stop Words Removal\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"],"metadata":{"id":"2yI494i4scex","executionInfo":{"status":"ok","timestamp":1714794129136,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Stemming\n","porter_stemmer = PorterStemmer()\n","stemmed_tokens = [porter_stemmer.stem(word) for word in filtered_tokens]"],"metadata":{"id":"Kmeu4xfGseK5","executionInfo":{"status":"ok","timestamp":1714794141882,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"],"metadata":{"id":"IxWTZgGHshPi","executionInfo":{"status":"ok","timestamp":1714794156326,"user_tz":-330,"elapsed":2726,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# TF-IDF Representation\n","documents = [sample_document]\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n","feature_names = tfidf_vectorizer.get_feature_names_out()"],"metadata":{"id":"Kc3mHHTpskIg","executionInfo":{"status":"ok","timestamp":1714794162646,"user_tz":-330,"elapsed":613,"user":{"displayName":"Rishi","userId":"06840069578059017882"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Display Results\n","print(\"Original Document:\\n\", sample_document, \"\\n\")\n","print(\"Tokenization:\\n\", tokens, \"\\n\")\n","print(\"POS Tagging:\\n\", pos_tags, \"\\n\")\n","print(\"Stop Words Removal:\\n\", filtered_tokens, \"\\n\")\n","print(\"Stemming:\\n\", stemmed_tokens, \"\\n\")\n","print(\"Lemmatization:\\n\", lemmatized_tokens, \"\\n\")\n","print(\"TF-IDF Representation:\\n\", tfidf_matrix.toarray(), \"\\n\")\n","print(\"Feature Names:\\n\", feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqHSB1y3smOy","executionInfo":{"status":"ok","timestamp":1714794171773,"user_tz":-330,"elapsed":425,"user":{"displayName":"Rishi","userId":"06840069578059017882"}},"outputId":"9f82925b-82cd-4a61-cadf-fe91937f7081"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Document:\n"," Natural language processing (NLP) is a subfield of artificial intelligence (AI) that\n","focuses on the interaction between computers and humans using natural language. It enables computers\n","to understand, interpret, and generate human-like text. NLP involves various tasks, such as tokenization,\n","part-of-speech tagging, stop words removal, stemming, and lemmatization. \n","\n","Tokenization:\n"," ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'It', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human-like', 'text', '.', 'NLP', 'involves', 'various', 'tasks', ',', 'such', 'as', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.'] \n","\n","POS Tagging:\n"," [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human-like', 'JJ'), ('text', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('involves', 'VBZ'), ('various', 'JJ'), ('tasks', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')] \n","\n","Stop Words Removal:\n"," ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focuses', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'enables', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human-like', 'text', '.', 'NLP', 'involves', 'various', 'tasks', ',', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.'] \n","\n","Stemming:\n"," ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'artifici', 'intellig', '(', 'ai', ')', 'focus', 'interact', 'comput', 'human', 'use', 'natur', 'languag', '.', 'enabl', 'comput', 'understand', ',', 'interpret', ',', 'gener', 'human-lik', 'text', '.', 'nlp', 'involv', 'variou', 'task', ',', 'token', ',', 'part-of-speech', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.'] \n","\n","Lemmatization:\n"," ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focus', 'interaction', 'computer', 'human', 'using', 'natural', 'language', '.', 'enables', 'computer', 'understand', ',', 'interpret', ',', 'generate', 'human-like', 'text', '.', 'NLP', 'involves', 'various', 'task', ',', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.'] \n","\n","TF-IDF Representation:\n"," [[0.12309149 0.36927447 0.12309149 0.12309149 0.12309149 0.24618298\n","  0.12309149 0.12309149 0.12309149 0.12309149 0.12309149 0.12309149\n","  0.12309149 0.12309149 0.12309149 0.12309149 0.12309149 0.24618298\n","  0.12309149 0.12309149 0.24618298 0.24618298 0.24618298 0.12309149\n","  0.12309149 0.12309149 0.12309149 0.12309149 0.12309149 0.12309149\n","  0.12309149 0.12309149 0.12309149 0.12309149 0.12309149 0.12309149\n","  0.12309149 0.12309149 0.12309149 0.12309149 0.12309149 0.12309149\n","  0.12309149]] \n","\n","Feature Names:\n"," ['ai' 'and' 'artificial' 'as' 'between' 'computers' 'enables' 'focuses'\n"," 'generate' 'human' 'humans' 'intelligence' 'interaction' 'interpret'\n"," 'involves' 'is' 'it' 'language' 'lemmatization' 'like' 'natural' 'nlp'\n"," 'of' 'on' 'part' 'processing' 'removal' 'speech' 'stemming' 'stop'\n"," 'subfield' 'such' 'tagging' 'tasks' 'text' 'that' 'the' 'to'\n"," 'tokenization' 'understand' 'using' 'various' 'words']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Mqc3tNB5soka"},"execution_count":null,"outputs":[]}]}